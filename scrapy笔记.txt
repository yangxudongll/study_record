scrapy建立步骤
	1.scrapy startproject name
	2.scrapy genspider name url (非必须)
	3.scrapy crawl name (-o filename输出到文件)


Item对象是一个简单的容器，用于收集抓取到的数据
	1、在items.py中定义类，如
		class QidianHotItem(scrapy.Item):
		    name=scrapy.Field()
		    author=scrapy.Field()
		    type=scrapy.Field()
		    form=scrapy.Field()
	2、在主文件中导入模块
		from qidian_hot.items import QidianHotItem
	3、使用
	    item=QidianHotItem()
        item["name"]=name
        item["author"]=author
        item["type"]=type
        item["form"]=form

使用ItemLoader填充容器
	1、导入ItemLoader类
		from scrapy.loader import ItemLoader
	2、实例化ItemLoader对象
		novel=ItemLoader(item=QidianHotItem(),selector=one_selector)
        novel.add_xpath("name","h4/a/text()")
        novel.add_xpath("author","p[1]/a[1]/text()")
        novel.add_xpath("type","p[1]/a[2]/text()")
        novel.add_xpath("form","p[1]/span/text()")
        yield novel.load_item()
    3、使用ItemLoader填充数据
    	ItemLoader提供了3种方式填充数据
    	.add_xpath()    #使用xpath选择器提取数据
    	.add_css() 
    	.add_value()    #直接传值，比如.add_value("type","连载")
进一步处理数据,在item.py中实现
	使用输入处理器和输出处理器对数据的输入和输出进一步解析
	from scrapy.loader.processors import TakeFirst

	def form_convert(form):
    if(form[0]=="连载"):
        return "lz"
    else:
        return "wj"

    class QidianHotItem(scrapy.Item):
	    name=scrapy.Field(output_processor=TakeFirst())

	    author=scrapy.Field(output_processor=TakeFirst())

	    type=scrapy.Field(output_processor=TakeFirst())
	   #注意,这里修改数据的格式,处理函数是在这个类外面实现的
	    form=scrapy.Field(input_processor=form_convert,output_processor=TakeFirst())

  	函数TakeFirst()为scrapy的内置函数，用于获取集合中的第一个非空值

  	上面的功能也可放在pipeline.py中实现


命令行创建spider文件和spider类
	在项目目录下输入 scrapy genspider name https://su.lianjia.com/ershoufang/
	其中name为爬虫名称，https://su.lianjia.com/ershoufang/  为初始访问url
user-agent
'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/77.0.3865.120 Safari/537.36'

设置请求时延
	DOWNLOAD_DELAY = 3

伪装浏览器，设置多个不同的请求头
	1、在setting.py中添加
		MY_USER_AGENT=[
		    'Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;AvantBrowser)',
		    'Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;360SE)',
		    'Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;Trident/4.0;SE2.XMetaSr1.0;SE2.XMetaSr1.0;.NETCLR2.0.50727;SE2.XMetaSr1.0)',
		    'Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;TheWorld)',
		    'Mozilla/4.0(compatible;MSIE7.0;WindowsNT5.1;TencentTraveler4.0)',
		    'Opera/9.80(Macintosh;IntelMacOSX10.6.8;U;en)Presto/2.8.131Version/11.11',
		    'Mozilla/5.0(WindowsNT6.1;rv:2.0.1)Gecko/20100101Firefox/4.0.1',
		    'Mozilla/5.0(compatible;MSIE9.0;WindowsNT6.1;Trident/5.0',
		    'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36 OPR/26.0.1656.60',
		    'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/30.0.1599.101 Safari/537.36',
		    'Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; QQDownload 732; .NET4.0C; .NET4.0E; LBBROWSER)',
		    'Mozilla/5.0 (Windows NT 5.1) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.84 Safari/535.11 SE 2.X MetaSr 1.0',
		    'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/38.0.2125.122 UBrowser/4.0.3214.0 Safari/537.36',
		    'Mozilla/5.0 (X11; U; Linux x86_64; zh-CN; rv:1.9.2.10) Gecko/20100922 Ubuntu/10.10 (maverick) Firefox/3.6.10',
		    'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.835.163 Safari/535.1',
		    "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)",
		    "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)",
		    "Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)",
		    "Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)",
		    "Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)",
		    "Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)",
		    "Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)",
		    "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)",
		    "Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6",
		    "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1",
		    "Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0",
		    "Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5",
		    "Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6",
		    "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11",
		    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20",
		    "Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52",
		]
	2、在middleware.py中添加类和方法
		class shetuUserAgentMiddleware(UserAgentMiddleware):
    		def process_request(self, request, spider):
        		agent=random.choice(list(MY_USER_AGENT))
       			request.headers.setdefault('User-Agent',agent)
       要先在头部导入
       	from scrapy.downloadermiddlewares.useragent import UserAgentMiddleware
		import random
		from shetu_download.settings import MY_USER_AGENT

	3、在setting.py中启用中间件
		DOWNLOADER_MIDDLEWARES = {
 		   'shetu_download.middlewares.shetuUserAgentMiddleware':100,
		}

	更简单方法
	1、导入
		from fake_useragent import UserAgent
	2、使用，中间件
		class shetuUserAgentMiddleware(UserAgentMiddleware):
		    def process_request(self, request, spider):
		        ua=UserAgent()
		        request.headers['User-Agent']=ua.random()
代理
	代理网站
		www.xicidaili.com
		www.kuaidaili.com



写入到其他文件
	1、在Pipeline.py中添加一个类，如
		class SavetoTxtPipeline(object):
		    file_name="xici.txt"
		    file=None
		    def open_spider(self,spider):
		        #打开文件
		        self.file=open(self.file_name,"a",encoding="utf-8")

		    def process_item(self,item,spider):
		        url=item["url"]
		        self.file.write(url+"\n")
		        return item

		    def close_spider(self,spider):
		        #关闭文件
		        self.file.close()
		特别注意，3个方法的名字固定
			open_spider(self,spider)
			process_spider(self,item,spider)
			close_spider(self,spider)
		还有一个访问核心组件的方法
			from_crawler(self,crawler)
			该方法被调用时，会创建一个新的Item Pipeline 对象，参数crawler为使用当前管道的项目。
			该方法常用于对核心组件的访问，如setting.py
	2、在setting.py中配置该方法
		ITEM_PIPELINES = {
 		   'xici_proxy.pipelines.SavetoTxtPipeline':300,
		}

文件下载
		文件管道FilesPipeline用于文件的下载
		把要下载的url保存到item()中，item["file_urls"],引擎会将Item传入FilePipeline进行下载，在下载完之前，Item处于锁定状态
		下载完之后，会将文件的信息收集到一个列表中，并赋值给key为files的Item字段
	启用管道
		ITEM_PIPELINES = {
		    'scrapy.pipelines.files.FilesPipeline':300,
		}

	重写FilePipeline
		from scrapy.pipelines.files import  FilesPipeline
		class SavetoTxtPipeline(FilesPipeline):
		    def file_path(self,request,response=None,info=None):
		        #从路径中获取文件名
		        file_name=request.url.split("/")[-1]
		        #文件夹名
		        folder_name=file_name.split('.')[0]
		        return  folder_name+"/"+file_name
	具体情况自己处理

图片下载
	图片管道ImagesPipeline
	ImagesPipeline继承于FilesPipeline，用法基本一致，区别如下
	导入路径
		scrapy.pipelines.images.ImagesPipeline
    下载目录
    	IMAGES_STORE
    Item中地址
    	image_url=scrapy.Field()
    
数据库
	1 在settings.py中配置数据库信息
		MYSQL_DB_NAME="mysql"
		MYSQL_HOST="localhost"
		MYSQL_USER="root"
		MYSQL_PASSWORD=''
	2 在pipeline.py中新建一个类,特别注意,三个函数名千万不能错
		class MySQLPipeline(object):
		    def open_spider(self,spider):                      //链接数据库
		        db_name=spider.settings["MYSQL_DB_NAME"]
		        host=spider.settings["MYSQL_HOST"]
		        user=spider.settings["MYSQL_USER"]
		        pwd=spider.settings["MYSQL_PASSWORD"]           //也可用spider.settings.get("MYSQL_PASSWORD")
		        self.db_conn=MySQLdb.connect(db=db_name,host=host,user=user,password=pwd,charset="utf8")
		        self.db_cursor=self.db_conn.cursor()

		    def process_item(self,item,spider):                //处理数据库事物
		        values=(item["name"],
		                item["author"],
		                item["type"],
		                item["form"])
		        sql="insert into novel(name,author,type,form) values(%s,%s,%s,%s)"
		        self.db_cursor.execute(sql,values)
		        return item

		    def close_spider(self,spider):                     //关闭数据库
		        #提交数据
		        self.db_conn.commit()
		        #关闭游标
		        #如果不想提交,用self.db_conn.rollback()回滚操作
		        self.db_cursor.close()
		        #关闭数据库
		        self.db_conn.close()
	3 在settings.py中启用
		ITEM_PIPELINES = {
		    'xici_proxy.pipelines.MySQLPipeline':400,
		}

注意到了吗?在pipeline中的每个类的三个函数都是
	def open_spider(self,spider)
	def process_item(self,item,spider)
	def close_spider(self,spider)


Request()函数的参数
	url  请求网址
	header         #如果在settings.py中有设置这里就没必要了
	callback  回调函数
	method  请求方法   GET  POST  PUT
	body    HTTP请求体  str/unicode
	cookies
	encoding
	meta  字典类型,用于数据传递
	priority  优先级  默认为0
	dont_filter    忽略重复请求,默认False
	errback    异常时调用的函数


selenium 使用
	Chrome

        chromedriver 下载: http://chromedriver.storage.googleapis.com/index.html

        安装

	$ unzip chromedriver_linux64.zip

	$ sudo mv chromedriver /usr/bin/

	$ cd /usr/bin/

	$ sudo chmod +x chromedriver


